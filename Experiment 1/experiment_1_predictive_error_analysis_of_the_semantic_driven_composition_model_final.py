# -*- coding: utf-8 -*-
"""Experiment_1_Predictive_Error_Analysis_of_the_Semantic-Driven_Composition_model_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_JxHKfZKXr9dljP1v-X_EoeNQTsjFVhb

**MLaaS Training Model**
---
"""

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Reload datasets
df_cifar = pd.read_csv("/content/CIFAR10_Federated_Combinations_Realistic.csv").assign(Dataset="CIFAR")
df_fmnist = pd.read_csv("/content/Fashion_MNIST_Federated_Combinations_Realistic.csv").assign(Dataset="F-MNIST")
df_mnist = pd.read_csv("/content/MNIST_Federated_Combinations_Realistic.csv").assign(Dataset="MNIST")
df_all = pd.concat([df_cifar, df_fmnist, df_mnist], ignore_index=True)
df_all

df_mnist.columns

df_all.columns

df_all.to_csv("all_datasets.csv", index=False)

"""**Semantic-Driven Composability rules(Combination-2)**
---
"""

import pandas as pd
import numpy as np
# Load dataset
df = pd.read_csv("all_datasets.csv")
# Filter: only length-2 combinations
df2 = df[df["Combination"].str.count("-") == 1].copy()

# --- Helper: normalized label vectors ---
def norm_labels(row, prefix):
    v = row[[f"{prefix}_Label{i}" for i in range(10)]].values.astype(float)
    s = v.sum()
    return v / s if s != 0 else v

# --- Compute all three scores ---
def compute_scores(row):
    # 1. Data Composability Score (Euclidean distance between normalized label distributions)
    v1 = norm_labels(row, "C1")
    v2 = norm_labels(row, "C2")
    data_score = float(np.sqrt(np.sum((v1 - v2) ** 2)))

    # 2. Scalability Score (Latency ratio â€” overhead factor)
    lat1 = float(row["C1_Latency(ms)"])
    lat2 = float(row["C2_Latency(ms)"])
    scalability_score = (lat1 + lat2) / lat1 if lat1 != 0 else np.nan

    # 3. Accuracy Similarity Score (between two individual accuracies)
    acc1 = float(row["C1_Accuracy(%)"])
    acc2 = float(row["C2_Accuracy(%)"])
    acc_sim = 1 - (abs(acc1 - acc2) / max(acc1, acc2)) if max(acc1, acc2) != 0 else np.nan

    return pd.Series({
        "Data_Composability_Score": data_score,
        "Scalability_Score": scalability_score,
        "Accuracy_Similarity_Score": acc_sim
    })

# Apply to df2
scores = df2.apply(compute_scores, axis=1)
df2 = pd.concat([df2, scores], axis=1)
df2

from google.colab import drive
drive.mount('/content/drive')

df2.to_csv("DF2.csv")

"""**Semantic-Driven Composability rules(Combination-3)**
---
"""

import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv("all_datasets.csv")

# Filter rows with combination length = 3
df3 = df[df["Combination"].str.count("-") == 2].copy()

# Load df2 (2-combination data) for lookup of base values
df2 = df[df["Combination"].str.count("-") == 1].copy()

# Create a dataset-aware lookup for base combinations (MultiIndex: Combination + Dataset)
base_lookup = df2.set_index(["Combination", "Dataset"])[["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]]

# Helper function to get normalized label vectors
def norm_labels(row, prefix_list):
    summed = np.zeros(10)
    for prefix in prefix_list:
        cols = [f"{prefix}_Label{i}" for i in range(10)]
        existing = [c for c in cols if c in row.index]
        if existing:
            summed += row[existing].values.astype(float)
    total = summed.sum()
    return summed / total if total != 0 else summed

# Compute composability metrics for 3-combination rows
def compute_d3(row):
    clients = row["Combination"].split("-")
    base_combo = "-".join(clients[:2])
    added_client = clients[2]
    dataset_name = row["Dataset"]

    # --- Data composability ---
    h_base = norm_labels(row, clients[:2])
    h_add = norm_labels(row, [added_client])
    data_score = float(np.sqrt(np.sum((h_base - h_add) ** 2)))

    # --- Scalability & Accuracy similarity ---
    try:
        base_metrics = base_lookup.loc[(base_combo, dataset_name)]
        lat_base = float(base_metrics["Global_Latency_Sum(ms)"])
        acc_base = float(base_metrics["Global_Accuracy(%)"])
    except KeyError:
        lat_base, acc_base = np.nan, np.nan

    lat_now = float(row["Global_Latency_Sum(ms)"])
    acc_now = float(row["Global_Accuracy(%)"])

    scalability_score = lat_now / lat_base if lat_base not in [0, np.nan] else np.nan
    acc_sim = 1 - (abs(acc_base - acc_now) / max(acc_base, acc_now)) if max(acc_base, acc_now) != 0 else np.nan

    return pd.Series({
        "Data_Composability_Score": data_score,
        "Scalability_Score": scalability_score,
        "Accuracy_Similarity_Score": acc_sim
    })

# Apply calculations
df3_scores = df3.apply(compute_d3, axis=1)
df3 = pd.concat([df3, df3_scores], axis=1)
df3

df3.to_csv("DF3.csv", index=False)

"""**Semantic-Driven Composability rules(Combination-4)**
---
"""

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("all_datasets.csv")

# Filter rows with combination length = 4
df4 = df[df["Combination"].str.count("-") == 3].copy()

# Load df3 (3-combination data) for lookup of base values
df3 = df[df["Combination"].str.count("-") == 2].copy()

# Create dataset-aware lookup for base combinations (MultiIndex: Combination + Dataset)
base_lookup = df3.set_index(["Combination", "Dataset"])[["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]]

# Helper function for normalized label vectors
def norm_labels(row, prefix_list):
    summed = np.zeros(10)
    for prefix in prefix_list:
        cols = [f"{prefix}_Label{i}" for i in range(10)]
        existing = [c for c in cols if c in row.index]
        if existing:
            summed += row[existing].values.astype(float)
    total = summed.sum()
    return summed / total if total != 0 else summed

# Compute composability metrics for 4-combination rows
def compute_d4(row):
    clients = row["Combination"].split("-")
    base_combo = "-".join(clients[:3])      # base = first three clients
    added_client = clients[3]               # added = last one
    dataset_name = row["Dataset"]

    # --- Data composability ---
    h_base = norm_labels(row, clients[:3])
    h_add = norm_labels(row, [added_client])
    data_score = float(np.sqrt(np.sum((h_base - h_add) ** 2)))

    # --- Scalability & Accuracy similarity ---
    try:
        base_metrics = base_lookup.loc[(base_combo, dataset_name)]
        lat_base = float(base_metrics["Global_Latency_Sum(ms)"])
        acc_base = float(base_metrics["Global_Accuracy(%)"])
    except KeyError:
        lat_base, acc_base = np.nan, np.nan

    lat_now = float(row["Global_Latency_Sum(ms)"])
    acc_now = float(row["Global_Accuracy(%)"])

    scalability_score = lat_now / lat_base if lat_base not in [0, np.nan] else np.nan
    acc_sim = 1 - (abs(acc_base - acc_now) / max(acc_base, acc_now)) if max(acc_base, acc_now) != 0 else np.nan

    return pd.Series({
        "Data_Composability_Score": data_score,
        "Scalability_Score": scalability_score,
        "Accuracy_Similarity_Score": acc_sim
    })

# Apply calculations
df4_scores = df4.apply(compute_d4, axis=1)
df4 = pd.concat([df4, df4_scores], axis=1)

# View results
print(df4[["Combination", "Dataset", "Data_Composability_Score",
            "Scalability_Score", "Accuracy_Similarity_Score"]].head(10))

df4

df4.to_csv("DF4.csv", index=False)

"""**Semantic-Driven Composability rules(Combination-5)**
---
"""

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("all_datasets.csv")

# ---------- Helper functions ----------
def norm_labels_from_prefixes(row, prefixes):
    """Sum label histograms for given client prefixes and return normalized distribution."""
    summed = np.zeros(10)
    for p in prefixes:
        cols = [f"{p}_Label{i}" for i in range(10)]
        existing = [c for c in cols if c in row.index]
        if existing:
            summed += row[existing].values.astype(float)
    total = summed.sum()
    return summed / total if total != 0 else summed

def safe_get(row, col):
    return float(row[col]) if col in row.index and pd.notna(row[col]) else np.nan

def sim_from_values(a, b):
    if pd.isna(a) or pd.isna(b):
        return np.nan
    m = max(a, b)
    return 1 - (abs(a - b) / m) if m != 0 else np.nan

# ---------- Build dataset-aware lookup for base 3-combo global metrics ----------
df_len3 = df[df["Combination"].str.count("-") == 2].copy()
lookup3 = df_len3.set_index(["Combination", "Dataset"])[["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]]

# ---------- Filter to length-5 combinations ----------
df5 = df[df["Combination"].str.count("-") == 4].copy()

def compute_len5_scores_sum(row):
    clients = row["Combination"].split("-")
    base3 = "-".join(clients[:3])
    add1, add2 = clients[3], clients[4]
    dataset_name = row["Dataset"]

    # 1) Data Composability Score (SUM, not mean)
    h_base = norm_labels_from_prefixes(row, clients[:3])
    h_add1 = norm_labels_from_prefixes(row, [add1])
    h_add2 = norm_labels_from_prefixes(row, [add2])
    d1 = float(np.sqrt(np.sum((h_base - h_add1) ** 2)))
    d2 = float(np.sqrt(np.sum((h_base - h_add2) ** 2)))
    data_comp_sum = np.nansum([d1, d2])  # sum, ignores NaNs

    # 2) Scalability Score (SUM of per-candidate ratios)
    try:
        base_metrics = lookup3.loc[(base3, dataset_name)]
        lat_base = float(base_metrics["Global_Latency_Sum(ms)"])
        acc_base = float(base_metrics["Global_Accuracy(%)"])
    except KeyError:
        lat_base = np.nan
        acc_base = np.nan

    lat1 = safe_get(row, f"{add1}_Latency(ms)")
    lat2 = safe_get(row, f"{add2}_Latency(ms)")
    s1 = (lat_base + lat1) / lat_base if pd.notna(lat_base) and lat_base != 0 and pd.notna(lat1) else np.nan
    s2 = (lat_base + lat2) / lat_base if pd.notna(lat_base) and lat_base != 0 and pd.notna(lat2) else np.nan
    scalability_sum = np.nansum([s1, s2])  # sum, ignores NaNs

    # 3) Accuracy Similarity Score (SUM of similarities)
    acc1 = safe_get(row, f"{add1}_Accuracy(%)")
    acc2 = safe_get(row, f"{add2}_Accuracy(%)")
    a1 = sim_from_values(acc_base, acc1)
    a2 = sim_from_values(acc_base, acc2)
    acc_similarity_sum = np.nansum([a1, a2])  # sum, ignores NaNs

    return pd.Series({
        "Data_Composability_Score": data_comp_sum,
        "Scalability_Score": scalability_sum,
        "Accuracy_Similarity_Score": acc_similarity_sum
    })

# Apply to df5
scores5_sum = df5.apply(compute_len5_scores_sum, axis=1)
df5 = pd.concat([df5, scores5_sum], axis=1)
df5

df5.to_csv("DF5.csv", index=False)

# Reload datasets
df2 = pd.read_csv("/content/DF2.csv")
df3 = pd.read_csv("/content/DF3.csv")
df4 = pd.read_csv("/content/DF4.csv")
df5 = pd.read_csv("/content/DF5.csv")
df_all = pd.concat([df2,df3,df5])

df_all

df_all.isna().sum()

#df_all = df_all.drop(columns=["Unnamed: 0"])
df_all.to_csv("Zero_shot_Dataset.csv")

df_all.columns

df_all=pd.read_csv("Zero_shot_Dataset.csv")
df_all

import pandas as pd

# Assuming your dataframe is named df_all or df_att
# Example: df_filtered = df_all.copy()

# Filter rows where the 'Combination' has exactly 4 dashes (i.e., 5 clients)
df_5combos = df_all[df_all['Combination'].str.count("-") == 4].copy()

# Optional: Reset index if needed
df_5combos.reset_index(drop=True, inplace=True)

# Display result
print(f"Filtered dataframe shape: {df_5combos.shape}")
print(df_5combos[['Combination']].head())
df_5combos

df_all.to_csv("Zero_shot_Dataset_5.csv")

"""**Semantic-Driven-Composition Model**
---
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

# ================================
# 1. Load Dataset
# ================================
csv_path = "Zero_shot_Dataset.csv"
assert os.path.exists(csv_path), f"File not found: {csv_path}"
df = pd.read_csv(csv_path)

# ================================
# Target / Exclusions
# ================================
target_cols = [
    "Global_Accuracy(%)",
    "Global_Latency_Sum(ms)"
]

# Columns to drop from features
explicit_drop_cols = {
    "Combination",
    "Dataset",
    'Unnamed: 0',
    "Global_RoundTime_Max(ms)",
    "Global_DataVolume"
}

# Validate targets exist
missing_targets = [c for c in target_cols if c not in df.columns]
if missing_targets:
    raise ValueError(f"Missing target columns in CSV: {missing_targets}")

# ================================
# 2. Prepare Data
# ================================
def coerce_num(series):
    return pd.to_numeric(series, errors="coerce")

df_num = df.copy()
for c in df_num.columns:
    df_num[c] = coerce_num(df_num[c])

drop_cols = set(target_cols) | explicit_drop_cols
feature_cols = [
    c for c in df_num.columns
    if c not in drop_cols and pd.api.types.is_numeric_dtype(df_num[c])
]

X = df_num[feature_cols].copy().fillna(df_num[feature_cols].median(numeric_only=True))
Y = df_num[target_cols].copy().fillna(df_num[target_cols].median(numeric_only=True))
combos = df["Combination"] if "Combination" in df.columns else pd.Series([None]*len(df))
datasets = df["Dataset"] if "Dataset" in df.columns else pd.Series(["Unknown"]*len(df))

X.shape

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

# ================================
# 1. Load Dataset
# ================================
csv_path = "Zero_shot_Dataset.csv"
assert os.path.exists(csv_path), f"File not found: {csv_path}"
df = pd.read_csv(csv_path)

# ================================
# Target / Exclusions
# ================================
target_cols = [
    "Global_Accuracy(%)",
    "Global_Latency_Sum(ms)"
]

# Columns to drop from features
explicit_drop_cols = {
    "Combination",
    "Dataset",
    'Unnamed: 0',
    "Global_RoundTime_Max(ms)",
    "Global_DataVolume"
}

# Validate targets exist
missing_targets = [c for c in target_cols if c not in df.columns]
if missing_targets:
    raise ValueError(f"Missing target columns in CSV: {missing_targets}")

# ================================
# 2. Prepare Data
# ================================
def coerce_num(series):
    return pd.to_numeric(series, errors="coerce")

df_num = df.copy()
for c in df_num.columns:
    df_num[c] = coerce_num(df_num[c])

drop_cols = set(target_cols) | explicit_drop_cols
feature_cols = [
    c for c in df_num.columns
    if c not in drop_cols and pd.api.types.is_numeric_dtype(df_num[c])
]

X = df_num[feature_cols].copy().fillna(df_num[feature_cols].median(numeric_only=True))
Y = df_num[target_cols].copy().fillna(df_num[target_cols].median(numeric_only=True))
combos = df["Combination"] if "Combination" in df.columns else pd.Series([None]*len(df))
datasets = df["Dataset"] if "Dataset" in df.columns else pd.Series(["Unknown"]*len(df))

# ================================
# 3. Train-Test Split
# ================================
X_train, X_test, y_train, y_test, combos_train, combos_test, dataset_train, dataset_test = train_test_split(
    X.values, Y.values, combos.values, datasets.values, test_size=0.2, random_state=42
)

# ================================
# 4. Min-Max Scaling
# ================================
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train_s = scaler_X.fit_transform(X_train)
X_test_s  = scaler_X.transform(X_test)
y_train_s = scaler_y.fit_transform(y_train)
y_test_s  = scaler_y.transform(y_test)

# ================================
# 5. Define Model
# ================================
class TinyNet(nn.Module):
    def __init__(self, inp, outp):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(inp, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, outp)
        )
    def forward(self, x):
        return self.net(x)

model = TinyNet(X_train_s.shape[1], y_train_s.shape[1])
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

# ================================
# 6. Training
# ================================
Xtr = torch.tensor(X_train_s, dtype=torch.float32)
ytr = torch.tensor(y_train_s, dtype=torch.float32)
Xte = torch.tensor(X_test_s,  dtype=torch.float32)
yte = torch.tensor(y_test_s,  dtype=torch.float32)

EPOCHS = 120
for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()
    pred = model(Xtr)
    loss = criterion(pred, ytr)
    loss.backward()
    optimizer.step()

# ================================
# 7. Evaluation
# ================================
model.eval()
with torch.no_grad():
    pred_test_s = model(Xte).cpu().numpy()

pred_test = scaler_y.inverse_transform(pred_test_s)
y_test_inv = scaler_y.inverse_transform(yte.cpu().numpy())

# ================================
# 8. Overall Metrics
# ================================
mae  = mean_absolute_error(y_test_inv, pred_test, multioutput="raw_values")
mape = mean_absolute_percentage_error(y_test_inv, pred_test, multioutput="raw_values") * 100
r2   = [r2_score(y_test_inv[:, i], pred_test[:, i]) for i in range(y_test_inv.shape[1])]

metrics_df = pd.DataFrame({
    "Target": target_cols,
    "MAE": mae,
    "MAPE": mape,
    "R2": r2
}).round(4)

# ================================
# 8B. Dataset-wise Aggregated Metrics
# ================================
dataset_metrics = []
unique_datasets = np.unique(dataset_test)

for ds in unique_datasets:
    mask = dataset_test == ds
    y_true_ds = y_test_inv[mask]
    y_pred_ds = pred_test[mask]
    if len(y_true_ds) == 0:
        continue

    # Aggregate across all target dimensions
    mae_ds  = mean_absolute_error(y_true_ds, y_pred_ds)
    mape_ds = mean_absolute_percentage_error(y_true_ds, y_pred_ds) * 100
    r2_ds   = r2_score(y_true_ds.flatten(), y_pred_ds.flatten())

    dataset_metrics.append({
        "Dataset": ds,
        "MAE": mae_ds,
        "MAPE": mape_ds,
        "R2": r2_ds
    })

dataset_metrics_df = pd.DataFrame(dataset_metrics).round(4)

# ================================
# 9. Comparison DataFrame (with dataset info)
# ================================
comparison_df = pd.DataFrame({
    "Dataset": dataset_test,
    "Combination": combos_test,
    "Actual_Accuracy(%)":        y_test_inv[:, 0],
    "Predicted_Accuracy(%)":     pred_test[:, 0],
    "Actual_Latency_Sum(ms)":    y_test_inv[:, 1],
    "Predicted_Latency_Sum(ms)": pred_test[:, 1],
})

# ================================
# 10. Save + Display
# ================================
metrics_path = "enhanced_tinynet_metrics.csv"
preds_path   = "enhanced_tinynet_test_predictions.csv"
dataset_metrics_path = "datasetwise_tinynet_metrics.csv"

metrics_df.to_csv(metrics_path, index=False)
dataset_metrics_df.to_csv(dataset_metrics_path, index=False)
comparison_df.to_csv(preds_path, index=False)

print("âœ… Trained on:", csv_path)
print(f"Features used (X): {len(feature_cols)}")
print(f"Targets (Y): {target_cols}\n")
print("Overall Metrics (MAE, MAPE, RÂ²):\n", metrics_df)
print("\nDataset-wise Aggregated Metrics:\n", dataset_metrics_df)
print("\nFull test predictions saved to:", preds_path)

X.columns

comparison_df

comparison_df['Combination'].value_counts()

comparison_df

import re, random
import pandas as pd

# Load your file (adjust path if needed)
df = comparison_df

def parse_set(s):
    # robust: grabs all integers regardless of spacing or separators
    return frozenset(map(int, re.findall(r"-?\d+", str(s))))

# Parse and make an order-invariant key
df["_set"] = df["Combination"].apply(parse_set)
df["_key"] = df["_set"].apply(lambda s: tuple(sorted(s)))

# Drop exact duplicates w.r.t. element set (ignores order)
df_unique = df.drop_duplicates(subset=["_key"]).copy()

# Randomly pick up to 100 unique rows
random_seed = 42  # set for reproducibility (or remove for true randomness)
sample_n = min(1000, len(df_unique))
sampled = df_unique.sample(n=sample_n, random_state=random_seed)

# If you want them back as neat strings like "1,2,3"
sampled["normalized_combinations"] = sampled["_key"].apply(lambda t: ",".join(map(str, t)))

# Result: `sampled` holds your unique random 500
print(len(df), "rows ->", len(df_unique), "unique combos -> sampled", len(sampled))
sampled

"""**Semantic-Driven Predictions for new Combinations MINIST**
---
"""

import pandas as pd
import numpy as np

# Load the new dataset
path = "few_text_dataset_1MINIST.csv"
df = pd.read_csv(path).copy()
df

import pandas as pd
import numpy as np

# --- Helper: Parse combination strings into client IDs ---
def parse_combination(combo_str):
    if pd.isna(combo_str):
        return []
    return combo_str.split("-")

# --- Helper: Create prefix mapping based on order ---
def get_prefix_map(clients):
    return {client: f"C{i+1}" for i, client in enumerate(clients)}

# --- Helper: Get normalized label histogram for a prefix ---
def get_normalized_labels(row, prefix):
    try:
        labels = [row[f"{prefix}_Label{i}"] for i in range(10)]
        labels = np.array(labels, dtype=float)
        total = labels.sum()
        return labels / total if total > 0 else labels
    except KeyError:
        return np.zeros(10)

# --- Compute the three scores per row ---
def compute_scores(row):
    old_clients = parse_combination(row["Old_Combination"])
    new_clients = parse_combination(row["New_Combination"])

    # Determine newly added clients
    added_clients = [c for c in new_clients if c not in old_clients]

    # Prefix maps (based on New_Combination order)
    prefix_map = get_prefix_map(new_clients)
    old_prefixes = [prefix_map[c] for c in old_clients if c in prefix_map]
    new_prefixes = [prefix_map[c] for c in added_clients if c in prefix_map]

    # --- 1. Data Composability Score (Mean of distances) ---
    base_hist = np.zeros(10)
    for p in old_prefixes:
        base_hist += get_normalized_labels(row, p)
    base_hist_sum = base_hist.sum()
    base_hist = base_hist / base_hist_sum if base_hist_sum > 0 else base_hist

    distances = []
    for p in new_prefixes:
        added_hist = get_normalized_labels(row, p)
        added_sum = added_hist.sum()
        added_hist = added_hist / added_sum if added_sum > 0 else added_hist
        dist = np.linalg.norm(base_hist - added_hist)
        distances.append(dist)
    data_score = float(np.mean(distances)) if distances else np.nan

    # --- 2. Base Accuracy and Latency (mean) ---
    base_accs = []
    base_lats = []
    for p in old_prefixes:
        acc = row.get(f"{p}_Accuracy(%)", np.nan)
        lat = row.get(f"{p}_Latency(ms)", np.nan)
        if pd.notna(acc): base_accs.append(float(acc))
        if pd.notna(lat): base_lats.append(float(lat))
    base_acc = np.mean(base_accs) if base_accs else np.nan
    base_lat = np.mean(base_lats) if base_lats else np.nan

    # --- 3. Accuracy Similarity Score (mean) ---
    acc_sims = []
    for p in new_prefixes:
        acc = row.get(f"{p}_Accuracy(%)", np.nan)
        if pd.notna(acc) and pd.notna(base_acc) and base_acc != 0:
            sim = 1 - abs(base_acc - acc) / max(base_acc, acc)
            acc_sims.append(sim)
    acc_sim_score = float(np.mean(acc_sims)) if acc_sims else np.nan

    # --- 4. Scalability Score (mean) ---
    scal_scores = []
    for p in new_prefixes:
        lat = row.get(f"{p}_Latency(ms)", np.nan)
        if pd.notna(lat) and pd.notna(base_lat) and base_lat != 0:
            ratio = (base_lat + lat) / base_lat
            scal_scores.append(ratio)
    scal_score = float(np.mean(scal_scores)) if scal_scores else np.nan

    return pd.Series({
        "Data_Composability_Score": data_score,
        "Scalability_Score": scal_score,
        "Accuracy_Similarity_Score": acc_sim_score
    })

# --- Apply the scoring function ---
scored_df = df.copy()
scored_metrics = scored_df.apply(compute_scores, axis=1)
scored_df = pd.concat([scored_df, scored_metrics], axis=1)

# --- Save result ---
scored_df.to_csv("few_text_dataset_1MINIST_with_mean_scores.csv", index=False)
print("âœ… Scoring complete with means. File saved as 'few_text_dataset_1MINIST_with_mean_scores.csv'.")

scored_df.to_csv("few_text_dataset_minist.csv")

scored_df.columns

scored_df = scored_df.drop('Unnamed: 0', axis=1)

import os
import time
import pandas as pd
import torch
# Path to the new dataset
new_csv_path = "few_text_dataset_minist.csv"
assert os.path.exists(new_csv_path), f"File not found: {new_csv_path}"
# Load new dataset
df_new = pd.read_csv(new_csv_path)
# ================================
# Select only the relevant feature columns
# ================================
feature_cols = ['Unnamed: 0',
    'C1_DataVolume(MB)', 'C1_FeatureCount', 'C1_Accuracy(%)', 'C1_Latency(ms)',
    'C1_Label0', 'C1_Label1', 'C1_Label2', 'C1_Label3', 'C1_Label4', 'C1_Label5',
    'C1_Label6', 'C1_Label7', 'C1_Label8', 'C1_Label9',
    'C2_DataVolume(MB)', 'C2_FeatureCount', 'C2_Accuracy(%)', 'C2_Latency(ms)',
    'C2_Label0', 'C2_Label1', 'C2_Label2', 'C2_Label3', 'C2_Label4', 'C2_Label5',
    'C2_Label6', 'C2_Label7', 'C2_Label8', 'C2_Label9',
    'C3_DataVolume(MB)', 'C3_FeatureCount', 'C3_Accuracy(%)', 'C3_Latency(ms)',
    'C3_Label0', 'C3_Label1', 'C3_Label2', 'C3_Label3', 'C3_Label4', 'C3_Label5',
    'C3_Label6', 'C3_Label7', 'C3_Label8', 'C3_Label9',
    'C4_DataVolume(MB)', 'C4_FeatureCount', 'C4_Accuracy(%)', 'C4_Latency(ms)',
    'C4_Label0', 'C4_Label1', 'C4_Label2', 'C4_Label3', 'C4_Label4', 'C4_Label5',
    'C4_Label6', 'C4_Label7', 'C4_Label8', 'C4_Label9',
    'C5_DataVolume(MB)', 'C5_FeatureCount', 'C5_Accuracy(%)', 'C5_Latency(ms)',
    'C5_Label0', 'C5_Label1', 'C5_Label2', 'C5_Label3', 'C5_Label4', 'C5_Label5',
    'C5_Label6', 'C5_Label7', 'C5_Label8', 'C5_Label9',
    'Data_Composability_Score', 'Scalability_Score', 'Accuracy_Similarity_Score'
]
# Check that all required columns exist
missing_cols = [c for c in feature_cols if c not in df_new.columns]
if missing_cols:
    raise ValueError(f"Missing columns in new dataset: {missing_cols}")
# ================================
# Preprocess and Predict
# ================================
# Coerce numeric and fill missing
df_new_num = df_new[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)

# Apply same scaler used during training
X_new_s = scaler_X.transform(df_new_num)

# Convert to tensor
X_new_tensor = torch.tensor(X_new_s, dtype=torch.float32)

# Predict row by row and time it
model.eval()
preds_list = []
times_list = []

with torch.no_grad():
    for i in range(len(X_new_tensor)):
        input_tensor = X_new_tensor[i].unsqueeze(0)
        start_time = time.time()
        pred = model(input_tensor).cpu().numpy()
        end_time = time.time()
        preds_list.append(pred[0])
        times_list.append(end_time - start_time)

# Convert predictions and times to DataFrames
preds_new = scaler_y.inverse_transform(preds_list)
target_cols = ["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]
new_preds_df = pd.DataFrame(preds_new, columns=target_cols)
new_preds_df["Prediction_Time(s)"] = times_list  # Add timing info

# Always include 'Combination' column
if "Combination" in df_new.columns:
    new_preds_df.insert(0, "Combination", df_new["Combination"])
else:
    new_preds_df.insert(0, "Combination", [f"Row_{i}" for i in range(len(df_new))])

# Add 'New_Combination' column if it exists
if "New_Combination" in df_new.columns:
    new_preds_df["New_Combination"] = df_new["New_Combination"]

# Save predictions
output_path = "new_dataset_predictions.csv"
new_preds_df.to_csv(output_path, index=False)
print(f"âœ… Predictions completed for: {new_csv_path}")
print(f"Predictions saved to: {output_path}")
print("\nSample predictions:\n", new_preds_df.head(10))

new_preds_df

new_preds_df.to_csv("zero-shot_1.csv")

import pandas as pd
import re
new_preds_df.to_csv("zero-shot_1.csv")
# Load both CSVs
df_sim = pd.read_csv("/content/Simulated_Results_From_New_Combinations_ms.csv")
df_zero = pd.read_csv("zero-shot_1.csv")
df_zero

df_sim

import pandas as pd
import re
# Ensure required column exists
if "New_Combination" not in df.columns:
    raise ValueError("The CSV must have a column named 'New_Combination'.")

# Convert like "C7-C10-C16" â†’ "7-10-16"
def to_numerical_combo(combo):
    if pd.isna(combo):
        return None
    numbers = re.findall(r'C(\d+)', str(combo))
    return "-".join(numbers)
# Apply conversion
df_sim["Numerical_Combination"] = df_sim["New_Combination"].apply(to_numerical_combo)
# Show preview
df_sim
df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_MINIST.csv")

df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_MINIST.csv")

df_sim

# The issue likely arises because both datasets have duplicate combinations.
# Weâ€™ll ensure unique matches by dropping duplicates before merging.

# Convert list to tuple for matching
df_zero["Numeric_Tuple"] = df_zero["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))
df_sim["Numeric_Tuple"] = df_sim["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))

# Drop duplicates to ensure one-to-one matching
df_zero_unique = df_zero.drop_duplicates(subset=["Numeric_Tuple"])
df_sim_unique = df_sim.drop_duplicates(subset=["Numeric_Tuple"])

# Find intersection (only unique tuples that appear in both)
matched_tuples = set(df_zero_unique["Numeric_Tuple"]).intersection(set(df_sim_unique["Numeric_Tuple"]))

# Filter only those rows
df_zero_matched = df_zero_unique[df_zero_unique["Numeric_Tuple"].isin(matched_tuples)]
df_sim_matched = df_sim_unique[df_sim_unique["Numeric_Tuple"].isin(matched_tuples)]

# Merge one-to-one based on the unique tuple
merged_df = pd.merge(df_zero_matched, df_sim_matched, on="Numeric_Tuple", suffixes=("_zero", "_sim"))

# Save final merged result
output_path = "Matched_Combinations_Results_Unique.csv"
merged_df.to_csv(output_path, index=False)

output_path, merged_df.shape

merged_df.columns

merged_df

merged_df_Copu=merged_df

merged_df= merged_df_Copu.sample(n=350, random_state=42)

scored_df

merged_df

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_absolute_error,
    mean_absolute_percentage_error,
    r2_score,
    mean_squared_error,
    median_absolute_error,
    explained_variance_score,
    max_error
)

# --- SMAPE function ---
def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
def compute_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    medae = median_absolute_error(y_true, y_pred)
    evs = explained_variance_score(y_true, y_pred)
    smape_val = smape(y_true.values, y_pred.values)
    maxerr = max_error(y_true, y_pred)

    return mae, mape, mse, rmse, r2, medae, evs, smape_val, maxerr


metrics_acc = compute_metrics(y_true_acc, y_pred_acc)
metrics_lat = compute_metrics(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])
metrics_all = compute_metrics(pd.Series(y_true_all), pd.Series(y_pred_all))

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [metrics_acc[0], metrics_lat[0], metrics_all[0]],
    "MAPE": [metrics_acc[1], metrics_lat[1], metrics_all[1]],
    "MSE": [metrics_acc[2], metrics_lat[2], metrics_all[2]],
    "RMSE": [metrics_acc[3], metrics_lat[3], metrics_all[3]]
}).round(4)

metrics_df

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [mae_acc, mae_lat, mae_overall],
    "MAPE": [mape_acc, mape_lat, mape_overall],
}).round(4)
metrics_df

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [mae_acc, mae_lat, mae_overall],
    "MAPE": [mape_acc, mape_lat, mape_overall],
}).round(4)
metrics_df

0.2178 & 9.7835 &	0.0963 & 970.4149 & 970.4149

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# Calculate per-target metrics
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
# Stack the two sets of predictions into single arrays
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["MINIST"],
    "MAE": [mae_overall],
    "MAPE": [mape_overall],
    "R2": [r2_overall]
}).round(4)

metrics_df

merged_df.to_csv("dd1.csv")

scored_df.to_csv("dd2.csv")

import pandas as pd

# Load the provided CSVs again
dd1 = merged_df  # merged_df equivalent
dd2 = scored_df  # source df_scored_fixed equivalent

print(f"dd1: {len(dd1)} rows, dd2: {len(dd2)} rows")

# --- Step 1: Verify both have the same row count ---
if len(dd1) != len(dd2):
    raise ValueError("Row counts differ! Check alignment before concatenating.")

# --- Step 2: Extract only the columns we need from dd2 ---
historical_df = dd2[["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]].copy()

# Rename them as historical versions
historical_df = historical_df.rename(columns={
    "Global_Accuracy(%)": "Global_Accuracy(%)_history",
    "Global_Latency_Sum(ms)": "Global_Latency_Sum(ms)_history"
})

# --- Step 3: Concatenate side-by-side (row-aligned) ---
merged_df_updated = pd.concat([dd1.reset_index(drop=True),
                               historical_df.reset_index(drop=True)], axis=1)

# --- Step 4: Confirm shape and preview ---
print(f"âœ… Final merged_df_updated shape: {merged_df_updated.shape}")
print("âœ… Columns added:", [c for c in merged_df_updated.columns if "_history" in c])
print("\nPreview:")
print(merged_df_updated.head(10))

merged_df_updated

merged_df_updated.to_csv("Final_Results_MINIST_ms.csv")

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
merged_df=merged_df_updated
# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# Calculate per-target metrics
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
# Stack the two sets of predictions into single arrays
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["MINIST"],
    "MAE": [mae_overall],
    "MAPE": [mape_overall],
    "R2": [r2_overall]
}).round(4)

metrics_df

"""**Semantic-Driven Predictions for new Combinations FMINIST**
---
"""

import pandas as pd
import numpy as np

# Load the new dataset
path = "few_text_dataset_1FMINIST.csv"
df = pd.read_csv(path).copy()
df

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("few_text_dataset_1FMINIST.csv")  # Adjust path if needed

# --- Helper: Parse combination strings into client IDs ---
def parse_combination(combo_str):
    if pd.isna(combo_str):
        return []
    return combo_str.split("-")

# --- Helper: Create prefix mapping based on order ---
def get_prefix_map(clients):
    return {client: f"C{i+1}" for i, client in enumerate(clients)}

# --- Helper: Get normalized label histogram for a prefix ---
def get_normalized_labels(row, prefix):
    try:
        labels = [row[f"{prefix}_Label{i}"] for i in range(10)]
        labels = np.array(labels, dtype=float)
        total = labels.sum()
        return labels / total if total > 0 else labels
    except KeyError:
        return np.zeros(10)

# --- Compute the three scores per row ---
def compute_scores(row):
    old_clients = parse_combination(row["Old_Combination"])
    new_clients = parse_combination(row["New_Combination"])

    # Determine newly added clients
    added_clients = [c for c in new_clients if c not in old_clients]

    # Prefix maps (based on New_Combination order)
    prefix_map = get_prefix_map(new_clients)
    old_prefixes = [prefix_map[c] for c in old_clients if c in prefix_map]
    new_prefixes = [prefix_map[c] for c in added_clients if c in prefix_map]

    # --- 1. Data Composability Score (Mean of distances) ---
    base_hist = np.zeros(10)
    for p in old_prefixes:
        base_hist += get_normalized_labels(row, p)
    base_hist_sum = base_hist.sum()
    base_hist = base_hist / base_hist_sum if base_hist_sum > 0 else base_hist

    distances = []
    for p in new_prefixes:
        added_hist = get_normalized_labels(row, p)
        added_sum = added_hist.sum()
        added_hist = added_hist / added_sum if added_sum > 0 else added_hist
        dist = np.linalg.norm(base_hist - added_hist)
        distances.append(dist)
    data_score = float(np.mean(distances)) if distances else np.nan

    # --- 2. Base Accuracy and Latency (mean) ---
    base_accs = []
    base_lats = []
    for p in old_prefixes:
        acc = row.get(f"{p}_Accuracy(%)", np.nan)
        lat = row.get(f"{p}_Latency(ms)", np.nan)
        if pd.notna(acc): base_accs.append(float(acc))
        if pd.notna(lat): base_lats.append(float(lat))
    base_acc = np.mean(base_accs) if base_accs else np.nan
    base_lat = np.mean(base_lats) if base_lats else np.nan

    # --- 3. Accuracy Similarity Score (mean) ---
    acc_sims = []
    for p in new_prefixes:
        acc = row.get(f"{p}_Accuracy(%)", np.nan)
        if pd.notna(acc) and pd.notna(base_acc) and base_acc != 0:
            sim = 1 - abs(base_acc - acc) / max(base_acc, acc)
            acc_sims.append(sim)
    acc_sim_score = float(np.mean(acc_sims)) if acc_sims else np.nan

    # --- 4. Scalability Score (mean) ---
    scal_scores = []
    for p in new_prefixes:
        lat = row.get(f"{p}_Latency(ms)", np.nan)
        if pd.notna(lat) and pd.notna(base_lat) and base_lat != 0:
            ratio = (base_lat + lat) / base_lat
            scal_scores.append(ratio)
    scal_score = float(np.mean(scal_scores)) if scal_scores else np.nan

    return pd.Series({
        "Data_Composability_Score": data_score,
        "Scalability_Score": scal_score,
        "Accuracy_Similarity_Score": acc_sim_score
    })

# --- Apply the scoring function ---
scored_df = df.copy()
scored_metrics = scored_df.apply(compute_scores, axis=1)
scored_df = pd.concat([scored_df, scored_metrics], axis=1)

# --- Save result ---
scored_df.to_csv("few_text_dataset_1MINIST_with_mean_scores.csv", index=False)
print("âœ… Scoring complete with means. File saved as 'few_text_dataset_1MINIST_with_mean_scores.csv'.")

scored_df.to_csv("few_text_dataset_fminist.csv")

scored_df.columns

# ================================
# 11. Predict on New Unseen Dataset (with Combination column)
# ================================
import os
import pandas as pd
import torch

# Path to the new dataset
new_csv_path = "few_text_dataset_fminist.csv"
assert os.path.exists(new_csv_path), f"File not found: {new_csv_path}"

# Load new dataset
df_new = pd.read_csv(new_csv_path)

# ================================
# Select only the relevant feature columns
# ================================
feature_cols = ['Unnamed: 0.1','Unnamed: 0',
    'C1_DataVolume(MB)', 'C1_FeatureCount', 'C1_Accuracy(%)', 'C1_Latency(ms)',
    'C1_Label0', 'C1_Label1', 'C1_Label2', 'C1_Label3', 'C1_Label4', 'C1_Label5',
    'C1_Label6', 'C1_Label7', 'C1_Label8', 'C1_Label9',
    'C2_DataVolume(MB)', 'C2_FeatureCount', 'C2_Accuracy(%)', 'C2_Latency(ms)',
    'C2_Label0', 'C2_Label1', 'C2_Label2', 'C2_Label3', 'C2_Label4', 'C2_Label5',
    'C2_Label6', 'C2_Label7', 'C2_Label8', 'C2_Label9',
    'C3_DataVolume(MB)', 'C3_FeatureCount', 'C3_Accuracy(%)', 'C3_Latency(ms)',
    'C3_Label0', 'C3_Label1', 'C3_Label2', 'C3_Label3', 'C3_Label4', 'C3_Label5',
    'C3_Label6', 'C3_Label7', 'C3_Label8', 'C3_Label9',
    'C4_DataVolume(MB)', 'C4_FeatureCount', 'C4_Accuracy(%)', 'C4_Latency(ms)',
    'C4_Label0', 'C4_Label1', 'C4_Label2', 'C4_Label3', 'C4_Label4', 'C4_Label5',
    'C4_Label6', 'C4_Label7', 'C4_Label8', 'C4_Label9',
    'C5_DataVolume(MB)', 'C5_FeatureCount', 'C5_Accuracy(%)', 'C5_Latency(ms)',
    'C5_Label0', 'C5_Label1', 'C5_Label2', 'C5_Label3', 'C5_Label4', 'C5_Label5',
    'C5_Label6', 'C5_Label7', 'C5_Label8', 'C5_Label9',
    'Data_Composability_Score', 'Scalability_Score', 'Accuracy_Similarity_Score'
]

# Check that all required columns exist
missing_cols = [c for c in feature_cols if c not in df_new.columns]
if missing_cols:
    raise ValueError(f"Missing columns in new dataset: {missing_cols}")

# ================================
# Preprocess and Predict
# ================================
# Coerce numeric and fill missing
df_new_num = df_new[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)

# Apply same scaler used during training
X_new_s = scaler_X.transform(df_new_num)

# Convert to tensor
X_new_tensor = torch.tensor(X_new_s, dtype=torch.float32)

# Predict using trained model
model.eval()
with torch.no_grad():
    preds_new_s = model(X_new_tensor).cpu().numpy()

# Inverse transform predictions to original scale
preds_new = scaler_y.inverse_transform(preds_new_s)

# ================================
# Build and Save Prediction DataFrame
# ================================
target_cols = ["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]
new_preds_df = pd.DataFrame(preds_new, columns=target_cols)

# Always include 'Combination' column
if "Combination" in df_new.columns:
    new_preds_df.insert(0, "Combination", df_new["Combination"])
else:
    new_preds_df.insert(0, "Combination", [f"Row_{i}" for i in range(len(df_new))])

# Add 'New_Combination' column if it exists
if "New_Combination" in df_new.columns:
    new_preds_df["New_Combination"] = df_new["New_Combination"]

# Save predictions
output_path = "new_dataset_predictions.csv"
new_preds_df.to_csv(output_path, index=False)

print(f"âœ… Predictions completed for: {new_csv_path}")
print(f"Predictions saved to: {output_path}")
print("\nSample predictions:\n", new_preds_df.head(10))

new_preds_df

new_preds_df.to_csv("zero-shot_2.csv")

import pandas as pd
import re

# Load both CSVs
df_sim = pd.read_csv("/content/Simulated_Results_From_New_Combinations_FMINIST.csv")
df_zero = pd.read_csv("zero-shot_2.csv")
df_zero

df_sim

import pandas as pd
import re
# Ensure required column exists
if "New_Combination" not in df.columns:
    raise ValueError("The CSV must have a column named 'New_Combination'.")

# Convert like "C7-C10-C16" â†’ "7-10-16"
def to_numerical_combo(combo):
    if pd.isna(combo):
        return None
    numbers = re.findall(r'C(\d+)', str(combo))
    return "-".join(numbers)
# Apply conversion
df_sim["Numerical_Combination"] = df_sim["New_Combination"].apply(to_numerical_combo)
# Show preview
df_sim

df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_FMINIST.csv")

df_sim

# The issue likely arises because both datasets have duplicate combinations.
# Weâ€™ll ensure unique matches by dropping duplicates before merging.

# Convert list to tuple for matching
df_zero["Numeric_Tuple"] = df_zero["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))
df_sim["Numeric_Tuple"] = df_sim["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))

# Drop duplicates to ensure one-to-one matching
df_zero_unique = df_zero.drop_duplicates(subset=["Numeric_Tuple"])
df_sim_unique = df_sim.drop_duplicates(subset=["Numeric_Tuple"])

# Find intersection (only unique tuples that appear in both)
matched_tuples = set(df_zero_unique["Numeric_Tuple"]).intersection(set(df_sim_unique["Numeric_Tuple"]))

# Filter only those rows
df_zero_matched = df_zero_unique[df_zero_unique["Numeric_Tuple"].isin(matched_tuples)]
df_sim_matched = df_sim_unique[df_sim_unique["Numeric_Tuple"].isin(matched_tuples)]

# Merge one-to-one based on the unique tuple
merged_df = pd.merge(df_zero_matched, df_sim_matched, on="Numeric_Tuple", suffixes=("_zero", "_sim"))

# Save final merged result
output_path = "Matched_Combinations_Results_Unique.csv"
merged_df.to_csv(output_path, index=False)

output_path, merged_df.shape

merged_df.columns

merged_df

scored_df

import pandas as pd

# Load the provided CSVs again
dd1 = merged_df  # merged_df equivalent
dd2 = scored_df  # source df_scored_fixed equivalent

print(f"dd1: {len(dd1)} rows, dd2: {len(dd2)} rows")

# --- Step 1: Verify both have the same row count ---
if len(dd1) != len(dd2):
    raise ValueError("Row counts differ! Check alignment before concatenating.")

# --- Step 2: Extract only the columns we need from dd2 ---
historical_df = dd2[["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]].copy()

# Rename them as historical versions
historical_df = historical_df.rename(columns={
    "Global_Accuracy(%)": "Global_Accuracy(%)_history",
    "Global_Latency_Sum(ms)": "Global_Latency_Sum(ms)_history"
})

# --- Step 3: Concatenate side-by-side (row-aligned) ---
merged_df_updated = pd.concat([dd1.reset_index(drop=True),
                               historical_df.reset_index(drop=True)], axis=1)

# --- Step 4: Confirm shape and preview ---
print(f"âœ… Final merged_df_updated shape: {merged_df_updated.shape}")
print("âœ… Columns added:", [c for c in merged_df_updated.columns if "_history" in c])
print("\nPreview:")
print(merged_df_updated.head(10))

merged_df_updated

merged_df_updated.to_csv("Final_results_of_FMINIST_ms.csv")

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [mae_acc, mae_lat, mae_overall],
    "MAPE": [mape_acc, mape_lat, mape_overall],
}).round(4)
metrics_df

"""**Semantic-Driven Predictions for new Combinations CIFAR10**
---
"""

import pandas as pd
import numpy as np

# Load the new dataset
path = "/content/few_text_dataset_1CIFAR10.csv"
df = pd.read_csv(path).copy()
df

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("few_text_dataset_1CIFAR10.csv")  # Adjust path if needed

# --- Helper: Parse combination strings into client IDs ---
def parse_combination(combo_str):
    if pd.isna(combo_str):
        return []
    return combo_str.split("-")

# --- Helper: Create prefix mapping based on order ---
def get_prefix_map(clients):
    return {client: f"C{i+1}" for i, client in enumerate(clients)}

# --- Helper: Get normalized label histogram for a prefix ---
def get_normalized_labels(row, prefix):
    try:
        labels = [row[f"{prefix}_Label{i}"] for i in range(10)]
        labels = np.array(labels, dtype=float)
        total = labels.sum()
        return labels / total if total > 0 else labels
    except KeyError:
        return np.zeros(10)

# --- Compute the three scores per row ---
def compute_scores(row):
    old_clients = parse_combination(row["Old_Combination"])
    new_clients = parse_combination(row["New_Combination"])

    # Determine newly added clients
    added_clients = [c for c in new_clients if c not in old_clients]

    # Prefix maps (based on New_Combination order)
    prefix_map = get_prefix_map(new_clients)
    old_prefixes = [prefix_map[c] for c in old_clients if c in prefix_map]
    new_prefixes = [prefix_map[c] for c in added_clients if c in prefix_map]

    # --- 1. Data Composability Score (Mean of distances) ---
    base_hist = np.zeros(10)
    for p in old_prefixes:
        base_hist += get_normalized_labels(row, p)
    base_hist_sum = base_hist.sum()
    base_hist = base_hist / base_hist_sum if base_hist_sum > 0 else base_hist

    distances = []
    for p in new_prefixes:
        added_hist = get_normalized_labels(row, p)
        added_sum = added_hist.sum()
        added_hist = added_hist / added_sum if added_sum > 0 else added_hist
        dist = np.linalg.norm(base_hist - added_hist)
        distances.append(dist)
    data_score = float(np.mean(distances)) if distances else np.nan

    # --- 2. Base Accuracy and Latency (mean) ---
    base_accs = []
    base_lats = []
    for p in old_prefixes:
        acc = row.get(f"{p}_Accuracy(%)", np.nan)
        lat = row.get(f"{p}_Latency(ms)", np.nan)
        if pd.notna(acc): base_accs.append(float(acc))
        if pd.notna(lat): base_lats.append(float(lat))
    base_acc = np.mean(base_accs) if base_accs else np.nan
    base_lat = np.mean(base_lats) if base_lats else np.nan

    # --- 3. Accuracy Similarity Score (mean) ---
    acc_sims = []
    for p in new_prefixes:
        acc = row.get(f"{p}_Accuracy(%)", np.nan)
        if pd.notna(acc) and pd.notna(base_acc) and base_acc != 0:
            sim = 1 - abs(base_acc - acc) / max(base_acc, acc)
            acc_sims.append(sim)
    acc_sim_score = float(np.mean(acc_sims)) if acc_sims else np.nan

    # --- 4. Scalability Score (mean) ---
    scal_scores = []
    for p in new_prefixes:
        lat = row.get(f"{p}_Latency(ms)", np.nan)
        if pd.notna(lat) and pd.notna(base_lat) and base_lat != 0:
            ratio = (base_lat + lat) / base_lat
            scal_scores.append(ratio)
    scal_score = float(np.mean(scal_scores)) if scal_scores else np.nan

    return pd.Series({
        "Data_Composability_Score": data_score,
        "Scalability_Score": scal_score,
        "Accuracy_Similarity_Score": acc_sim_score
    })

# --- Apply the scoring function ---
scored_df = df.copy()
scored_metrics = scored_df.apply(compute_scores, axis=1)
scored_df = pd.concat([scored_df, scored_metrics], axis=1)

# --- Save result ---
scored_df.to_csv("few_text_dataset_1MINIST_with_mean_scores.csv", index=False)
print("âœ… Scoring complete with means. File saved as 'few_text_dataset_1MINIST_with_mean_scores.csv'.")

scored_df.to_csv("few_text_dataset_CIFAR10.csv")

scored_df.columns

df_new

# ================================
# 11. Predict on New Unseen Dataset (with Combination column)
# ================================
import os
import pandas as pd
import torch

# Path to the new dataset
new_csv_path = "few_text_dataset_CIFAR10.csv"
assert os.path.exists(new_csv_path), f"File not found: {new_csv_path}"

# Load new dataset
df_new = pd.read_csv(new_csv_path)

# ================================
# Select only the relevant feature columns
# ================================
feature_cols = ['Unnamed: 0.1','Unnamed: 0.2',
    'C1_DataVolume(MB)', 'C1_FeatureCount', 'C1_Accuracy(%)', 'C1_Latency(ms)',
    'C1_Label0', 'C1_Label1', 'C1_Label2', 'C1_Label3', 'C1_Label4', 'C1_Label5',
    'C1_Label6', 'C1_Label7', 'C1_Label8', 'C1_Label9',
    'C2_DataVolume(MB)', 'C2_FeatureCount', 'C2_Accuracy(%)', 'C2_Latency(ms)',
    'C2_Label0', 'C2_Label1', 'C2_Label2', 'C2_Label3', 'C2_Label4', 'C2_Label5',
    'C2_Label6', 'C2_Label7', 'C2_Label8', 'C2_Label9',
    'C3_DataVolume(MB)', 'C3_FeatureCount', 'C3_Accuracy(%)', 'C3_Latency(ms)',
    'C3_Label0', 'C3_Label1', 'C3_Label2', 'C3_Label3', 'C3_Label4', 'C3_Label5',
    'C3_Label6', 'C3_Label7', 'C3_Label8', 'C3_Label9',
    'C4_DataVolume(MB)', 'C4_FeatureCount', 'C4_Accuracy(%)', 'C4_Latency(ms)',
    'C4_Label0', 'C4_Label1', 'C4_Label2', 'C4_Label3', 'C4_Label4', 'C4_Label5',
    'C4_Label6', 'C4_Label7', 'C4_Label8', 'C4_Label9',
    'C5_DataVolume(MB)', 'C5_FeatureCount', 'C5_Accuracy(%)', 'C5_Latency(ms)',
    'C5_Label0', 'C5_Label1', 'C5_Label2', 'C5_Label3', 'C5_Label4', 'C5_Label5',
    'C5_Label6', 'C5_Label7', 'C5_Label8', 'C5_Label9',
    'Data_Composability_Score', 'Scalability_Score', 'Accuracy_Similarity_Score'
]

# Check that all required columns exist
missing_cols = [c for c in feature_cols if c not in df_new.columns]
if missing_cols:
    raise ValueError(f"Missing columns in new dataset: {missing_cols}")

# ================================
# Preprocess and Predict
# ================================
# Coerce numeric and fill missing
df_new_num = df_new[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)

# Apply same scaler used during training
X_new_s = scaler_X.transform(df_new_num)

# Convert to tensor
X_new_tensor = torch.tensor(X_new_s, dtype=torch.float32)

# Predict using trained model
model.eval()
with torch.no_grad():
    preds_new_s = model(X_new_tensor).cpu().numpy()

# Inverse transform predictions to original scale
preds_new = scaler_y.inverse_transform(preds_new_s)

# ================================
# Build and Save Prediction DataFrame
# ================================
target_cols = ["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]
new_preds_df = pd.DataFrame(preds_new, columns=target_cols)

# Always include 'Combination' column
if "Combination" in df_new.columns:
    new_preds_df.insert(0, "Combination", df_new["Combination"])
else:
    new_preds_df.insert(0, "Combination", [f"Row_{i}" for i in range(len(df_new))])

# Add 'New_Combination' column if it exists
if "New_Combination" in df_new.columns:
    new_preds_df["New_Combination"] = df_new["New_Combination"]

# Save predictions
output_path = "new_dataset_predictions.csv"
new_preds_df.to_csv(output_path, index=False)

print(f"âœ… Predictions completed for: {new_csv_path}")
print(f"Predictions saved to: {output_path}")
print("\nSample predictions:\n", new_preds_df.head(10))

new_preds_df

new_preds_df.to_csv("zero-shot_3.csv")

import pandas as pd
import re

# Load both CSVs
df_sim = pd.read_csv("/content/Simulated_Results_From_New_Combinations_CIFAR10.csv")
df_zero = pd.read_csv("zero-shot_3.csv")
df_zero

df_sim

import pandas as pd
import re
# Ensure required column exists
if "New_Combination" not in df.columns:
    raise ValueError("The CSV must have a column named 'New_Combination'.")

# Convert like "C7-C10-C16" â†’ "7-10-16"
def to_numerical_combo(combo):
    if pd.isna(combo):
        return None
    numbers = re.findall(r'C(\d+)', str(combo))
    return "-".join(numbers)
# Apply conversion
df_sim["Numerical_Combination"] = df_sim["New_Combination"].apply(to_numerical_combo)
# Show preview
df_sim

df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_CIFAR10.csv")

df_sim

# The issue likely arises because both datasets have duplicate combinations.
# Weâ€™ll ensure unique matches by dropping duplicates before merging.

# Convert list to tuple for matching
df_zero["Numeric_Tuple"] = df_zero["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))
df_sim["Numeric_Tuple"] = df_sim["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))

# Drop duplicates to ensure one-to-one matching
df_zero_unique = df_zero.drop_duplicates(subset=["Numeric_Tuple"])
df_sim_unique = df_sim.drop_duplicates(subset=["Numeric_Tuple"])

# Find intersection (only unique tuples that appear in both)
matched_tuples = set(df_zero_unique["Numeric_Tuple"]).intersection(set(df_sim_unique["Numeric_Tuple"]))

# Filter only those rows
df_zero_matched = df_zero_unique[df_zero_unique["Numeric_Tuple"].isin(matched_tuples)]
df_sim_matched = df_sim_unique[df_sim_unique["Numeric_Tuple"].isin(matched_tuples)]

# Merge one-to-one based on the unique tuple
merged_df = pd.merge(df_zero_matched, df_sim_matched, on="Numeric_Tuple", suffixes=("_zero", "_sim"))

# Save final merged result
output_path = "Matched_Combinations_Results_Unique.csv"
merged_df.to_csv(output_path, index=False)

output_path, merged_df.shape

merged_df.columns

merged_df

scored_df

import pandas as pd

# Load the provided CSVs again
dd1 = merged_df  # merged_df equivalent
dd2 = scored_df  # source df_scored_fixed equivalent

print(f"dd1: {len(dd1)} rows, dd2: {len(dd2)} rows")

# --- Step 1: Verify both have the same row count ---
if len(dd1) != len(dd2):
    raise ValueError("Row counts differ! Check alignment before concatenating.")

# --- Step 2: Extract only the columns we need from dd2 ---
historical_df = dd2[["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]].copy()

# Rename them as historical versions
historical_df = historical_df.rename(columns={
    "Global_Accuracy(%)": "Global_Accuracy(%)_history",
    "Global_Latency_Sum(ms)": "Global_Latency_Sum(ms)_history"
})

# --- Step 3: Concatenate side-by-side (row-aligned) ---
merged_df_updated = pd.concat([dd1.reset_index(drop=True),
                               historical_df.reset_index(drop=True)], axis=1)

# --- Step 4: Confirm shape and preview ---
print(f"âœ… Final merged_df_updated shape: {merged_df_updated.shape}")
print("âœ… Columns added:", [c for c in merged_df_updated.columns if "_history" in c])
print("\nPreview:")
print(merged_df_updated.head(10))

merged_df_updated

merged_df_updated.to_csv("Final_results_of_CIFAR10_ms.csv")

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "CIFAR-10"],
    "MAE": [mae_acc, mae_lat, mae_overall],
    "MAPE": [mape_acc, mape_lat, mape_overall],

}).round(4)

metrics_df



import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
merged_df=merged_df_updated
# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# Calculate per-target metrics
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
# Stack the two sets of predictions into single arrays
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["CIFAR10"],
    "MAE": [mae_overall],
    "MAPE": [mape_overall],
    "R2": [r2_overall]
}).round(4)

metrics_df

merged_df=merged_df[['New_Combination_zero','Global_Accuracy(%)_sim','Global_Accuracy(%)_zero','Global_Latency_Sum(ms)_zero','Global_Latency_Sum(ms)_sim']]
merged_df

merged_df_updated.isna().sum()



"""**DT-QoS FMINIST**
---
"""

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

# ================================
# 1. Load Dataset
# ================================
csv_path = "Zero_shot_Dataset.csv"
assert os.path.exists(csv_path), f"File not found: {csv_path}"
df = pd.read_csv(csv_path)
# ================================
# Target / Exclusions
# ================================
target_cols = [
    "Global_Accuracy(%)",
    "Global_Latency_Sum(ms)"
]
# Columns to drop from features
explicit_drop_cols = {
    "Combination",
    "Dataset",
    "Unnamed: 0",
    "Global_RoundTime_Max(ms)",
    "Global_DataVolume"
}

# Validate targets exist
missing_targets = [c for c in target_cols if c not in df.columns]
if missing_targets:
    raise ValueError(f"Missing target columns in CSV: {missing_targets}")

# ================================
# 2. Prepare Data
# ================================
def coerce_num(series):
    return pd.to_numeric(series, errors="coerce")

df_num = df.copy()
for c in df_num.columns:
    df_num[c] = coerce_num(df_num[c])

drop_cols = set(target_cols) | explicit_drop_cols
feature_cols = [
    c for c in df_num.columns
    if c not in drop_cols and pd.api.types.is_numeric_dtype(df_num[c])
]
X = df_num[feature_cols].copy().fillna(df_num[feature_cols].median(numeric_only=True))
Y = df_num[target_cols].copy().fillna(df_num[target_cols].median(numeric_only=True))
combos = df["Combination"] if "Combination" in df.columns else pd.Series([None] * len(df))
datasets = df["Dataset"] if "Dataset" in df.columns else pd.Series(["Unknown"] * len(df))
# ================================
# 3. Train-Test Split
# ================================
X_train, X_test, y_train, y_test, combos_train, combos_test, dataset_train, dataset_test = train_test_split(
    X.values, Y.values, combos.values, datasets.values, test_size=0.2, random_state=42
)

# ================================
# 4. Decision Tree Regression Model
# ================================
# Note: DecisionTreeRegressor supports multi-output regression directly when y is 2D.
model = DecisionTreeRegressor(
    random_state=42,
    max_depth=None,        # you can tune this
    min_samples_split=2,   # you can tune this
    min_samples_leaf=1     # you can tune this
)

model.fit(X_train, y_train)

# ================================
# 5. Evaluation
# ================================
pred_test = model.predict(X_test)
y_test_inv = y_test  # no scaling, so "inv" is just y_test

# ================================
# 6. Overall Metrics (per target)
# ================================
mae  = mean_absolute_error(y_test_inv, pred_test, multioutput="raw_values")
mape = mean_absolute_percentage_error(y_test_inv, pred_test, multioutput="raw_values") * 100
r2   = [r2_score(y_test_inv[:, i], pred_test[:, i]) for i in range(y_test_inv.shape[1])]

metrics_df = pd.DataFrame({
    "Target": target_cols,
    "MAE": mae,
    "MAPE": mape,
    "R2": r2
}).round(4)

# ================================
# 6B. Dataset-wise Aggregated Metrics
# ================================
dataset_metrics = []
unique_datasets = np.unique(dataset_test)

for ds in unique_datasets:
    mask = dataset_test == ds
    y_true_ds = y_test_inv[mask]
    y_pred_ds = pred_test[mask]
    if len(y_true_ds) == 0:
        continue

    # Aggregate across all target dimensions
    mae_ds  = mean_absolute_error(y_true_ds, y_pred_ds)
    mape_ds = mean_absolute_percentage_error(y_true_ds, y_pred_ds) * 100
    r2_ds   = r2_score(y_true_ds.flatten(), y_pred_ds.flatten())

    dataset_metrics.append({
        "Dataset": ds,
        "MAE": mae_ds,
        "MAPE": mape_ds,
        "R2": r2_ds
    })

dataset_metrics_df = pd.DataFrame(dataset_metrics).round(4)

# ================================
# 7. Comparison DataFrame (with dataset info)
# ================================
comparison_df = pd.DataFrame({
    "Dataset": dataset_test,
    "Combination": combos_test,
    "Actual_Accuracy(%)":        y_test_inv[:, 0],
    "Predicted_Accuracy(%)":     pred_test[:, 0],
    "Actual_Latency_Sum(ms)":    y_test_inv[:, 1],
    "Predicted_Latency_Sum(ms)": pred_test[:, 1],
})

# ================================
# 8. Save + Display
# ================================
metrics_path = "decisiontree_metrics.csv"
preds_path   = "decisiontree_test_predictions.csv"
dataset_metrics_path = "datasetwise_decisiontree_metrics.csv"

metrics_df.to_csv(metrics_path, index=False)
dataset_metrics_df.to_csv(dataset_metrics_path, index=False)
comparison_df.to_csv(preds_path, index=False)

print("âœ… Trained on:", csv_path)
print(f"Features used (X): {len(feature_cols)}")
print(f"Targets (Y): {target_cols}\n")
print("Overall Metrics (MAE, MAPE, RÂ²):\n", metrics_df)
print("\nDataset-wise Aggregated Metrics:\n", dataset_metrics_df)
print("\nFull test predictions saved to:", preds_path)

import os
import time
import numpy as np
import pandas as pd

# ================================
# 1. Load New Dataset
# ================================
new_csv_path = "few_text_dataset_fminist.csv"
assert os.path.exists(new_csv_path), f"File not found: {new_csv_path}"
df_new = pd.read_csv(new_csv_path)

# ================================
# 2. Feature Columns (same as training)
# ================================
feature_cols = [
    'Unnamed: 0',
    'C1_DataVolume(MB)', 'C1_FeatureCount', 'C1_Accuracy(%)', 'C1_Latency(ms)',
    'C1_Label0', 'C1_Label1', 'C1_Label2', 'C1_Label3', 'C1_Label4', 'C1_Label5',
    'C1_Label6', 'C1_Label7', 'C1_Label8', 'C1_Label9',
    'C2_DataVolume(MB)', 'C2_FeatureCount', 'C2_Accuracy(%)', 'C2_Latency(ms)',
    'C2_Label0', 'C2_Label1', 'C2_Label2', 'C2_Label3', 'C2_Label4', 'C2_Label5',
    'C2_Label6', 'C2_Label7', 'C2_Label8', 'C2_Label9',
    'C3_DataVolume(MB)', 'C3_FeatureCount', 'C3_Accuracy(%)', 'C3_Latency(ms)',
    'C3_Label0', 'C3_Label1', 'C3_Label2', 'C3_Label3', 'C3_Label4', 'C3_Label5',
    'C3_Label6', 'C3_Label7', 'C3_Label8', 'C3_Label9',
    'C4_DataVolume(MB)', 'C4_FeatureCount', 'C4_Accuracy(%)', 'C4_Latency(ms)',
    'C4_Label0', 'C4_Label1', 'C4_Label2', 'C4_Label3', 'C4_Label4', 'C4_Label5',
    'C4_Label6', 'C4_Label7', 'C4_Label8', 'C4_Label9',
    'C5_DataVolume(MB)', 'C5_FeatureCount', 'C5_Accuracy(%)', 'C5_Latency(ms)',
    'C5_Label0', 'C5_Label1', 'C5_Label2', 'C5_Label3', 'C5_Label4', 'C5_Label5',
    'C5_Label6', 'C5_Label7', 'C5_Label8', 'C5_Label9',
    'Data_Composability_Score', 'Scalability_Score', 'Accuracy_Similarity_Score'
]

# Check all required columns exist
missing_cols = [c for c in feature_cols if c not in df_new.columns]
if missing_cols:
    raise ValueError(f"Missing columns in new dataset: {missing_cols}")

# ================================
# 3. Preprocess
# ================================
df_new_num = df_new[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
X_new = df_new_num.values

# If (and only if) you trained the Decision Tree using scaler_X,
# then apply it here too. Otherwise, delete the next line.
# X_new = scaler_X.transform(X_new)

# ================================
# 4. Predict row-by-row + timing
# ================================
preds_list = []
times_list = []

for i in range(len(X_new)):
    x_row = X_new[i].reshape(1, -1)
    start_time = time.time()
    pred = model.predict(x_row)          # shape (1, 2)
    end_time = time.time()

    preds_list.append(pred[0])
    times_list.append(end_time - start_time)

preds_arr = np.array(preds_list)

# ================================
# 5. Build Output DataFrame
# ================================
target_cols = ["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]
new_preds_df = pd.DataFrame(preds_arr, columns=target_cols)
new_preds_df["Prediction_Time(s)"] = times_list

# Always include 'Combination'
if "Combination" in df_new.columns:
    new_preds_df.insert(0, "Combination", df_new["Combination"])
else:
    new_preds_df.insert(0, "Combination", [f"Row_{i}" for i in range(len(df_new))])

# Include New_Combination if exists
if "New_Combination" in df_new.columns:
    new_preds_df["New_Combination"] = df_new["New_Combination"]

# ================================
# 6. Save + Print
# ================================
output_path = "new_dataset_predictions_decisiontree.csv"
new_preds_df.to_csv(output_path, index=False)

print(f"âœ… Decision Tree predictions completed for: {new_csv_path}")
print(f"Predictions saved to: {output_path}")
print("\nSample predictions:\n", new_preds_df.head(10))

import pandas as pd
import re
new_preds_df.to_csv("zero-shot_1.csv")
# Load both CSVs
df_sim = pd.read_csv("/content/Simulated_Results_From_New_Combinations_ms.csv")
df_zero = pd.read_csv("zero-shot_1.csv")
df_zero

df_sim

df_zero.rename(columns={"Combination": "'New_Combination"}, inplace=True)

import pandas as pd
import re
# Ensure required column exists
if "New_Combination" not in df.columns:
    raise ValueError("The CSV must have a column named 'New_Combination'.")

# Convert like "C7-C10-C16" â†’ "7-10-16"
def to_numerical_combo(combo):
    if pd.isna(combo):
        return None
    numbers = re.findall(r'C(\d+)', str(combo))
    return "-".join(numbers)
# Apply conversion
df_sim["Numerical_Combination"] = df_sim["New_Combination"].apply(to_numerical_combo)
# Show preview
df_sim
df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_MINIST.csv")

# The issue likely arises because both datasets have duplicate combinations.
# Weâ€™ll ensure unique matches by dropping duplicates before merging.

# Convert list to tuple for matching
df_zero["Numeric_Tuple"] = df_zero["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))
df_sim["Numeric_Tuple"] = df_sim["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))

# Drop duplicates to ensure one-to-one matching
df_zero_unique = df_zero.drop_duplicates(subset=["Numeric_Tuple"])
df_sim_unique = df_sim.drop_duplicates(subset=["Numeric_Tuple"])

# Find intersection (only unique tuples that appear in both)
matched_tuples = set(df_zero_unique["Numeric_Tuple"]).intersection(set(df_sim_unique["Numeric_Tuple"]))

# Filter only those rows
df_zero_matched = df_zero_unique[df_zero_unique["Numeric_Tuple"].isin(matched_tuples)]
df_sim_matched = df_sim_unique[df_sim_unique["Numeric_Tuple"].isin(matched_tuples)]

# Merge one-to-one based on the unique tuple
merged_df = pd.merge(df_zero_matched, df_sim_matched, on="Numeric_Tuple", suffixes=("_zero", "_sim"))

# Save final merged result
output_path = "Matched_Combinations_Results_Unique.csv"
merged_df.to_csv(output_path, index=False)

output_path, merged_df.shape

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_absolute_error,
    mean_absolute_percentage_error,
    r2_score,
    mean_squared_error,
    median_absolute_error,
    explained_variance_score,
    max_error
)

# --- SMAPE function ---
def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
def compute_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    medae = median_absolute_error(y_true, y_pred)
    evs = explained_variance_score(y_true, y_pred)
    smape_val = smape(y_true.values, y_pred.values)
    maxerr = max_error(y_true, y_pred)

    return mae, mape, mse, rmse, r2, medae, evs, smape_val, maxerr


metrics_acc = compute_metrics(y_true_acc, y_pred_acc)
metrics_lat = compute_metrics(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])
metrics_all = compute_metrics(pd.Series(y_true_all), pd.Series(y_pred_all))

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [metrics_acc[0], metrics_lat[0], metrics_all[0]],
    "MAPE": [metrics_acc[1], metrics_lat[1], metrics_all[1]],
    "MSE": [metrics_acc[2], metrics_lat[2], metrics_all[2]],
    "RMSE": [metrics_acc[3], metrics_lat[3], metrics_all[3]]
}).round(4)

metrics_df



import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
merged_df=merged_df_updated
# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# Calculate per-target metrics
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
# Stack the two sets of predictions into single arrays
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["FMINIST"],
    "MAE": [mae_overall],
    "MAPE": [mape_overall],
    "R2": [r2_overall]
}).round(4)

metrics_df

merged_df_updated.isna().sum()

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

# ===============================
# Load and prepare data
# ===============================
df = pd.read_csv("merged_with_historical.csv").dropna(
    subset=["ZeroShot_Accuracy(%)", "Simulated_Accuracy(%)"]
).reset_index(drop=True)

df["Combination_Index"] = range(len(df))

# ===============================
# Plot configuration
# ===============================
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_context("talk", font_scale=1.5)

plt.figure(figsize=(14, 7))

# Zero-shot predicted accuracy
sns.lineplot(
    x="Combination_Index", y="ZeroShot_Accuracy(%)", data=df,
    label="Zero-Shot Predicted", color="royalblue", linewidth=3
)

# Simulated (final predicted) accuracy
sns.lineplot(
    x="Combination_Index", y="Simulated_Accuracy(%)", data=df,
    label="Simulated Accuracy", color="darkorange", linewidth=3
)

# ===============================
# Formatting
# ===============================
plt.xlabel("Combination Index", fontsize=22, labelpad=15)
plt.ylabel("Accuracy (%)", fontsize=22, labelpad=15)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.grid(True, which='major', linestyle='--', linewidth=0.6, alpha=0.6)
plt.legend(fontsize=20, loc='best', frameon=False)
plt.tight_layout()

# ===============================
# Optional high-res save
# ===============================
# plt.savefig("zero_vs_simulated_accuracy_acm.pdf", dpi=600, bbox_inches='tight')

plt.show()

# ===============================
# Compute and print trend consistency
# ===============================
df["ZS_Trend"] = np.sign(df["ZeroShot_Accuracy(%)"].diff())
df["Sim_Trend"] = np.sign(df["Simulated_Accuracy(%)"].diff())
trend_agreement = (df["ZS_Trend"] == df["Sim_Trend"]).mean() * 100
print(f"Trend Consistency between Zero-Shot and Simulated Accuracies: {trend_agreement:.2f}%")



"""**DT-QoS CIFAR-10**
---
"""

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

# ================================
# 1. Load Dataset
# ================================
csv_path = "Zero_shot_Dataset.csv"
assert os.path.exists(csv_path), f"File not found: {csv_path}"
df = pd.read_csv(csv_path)
# ================================
# Target / Exclusions
# ================================
target_cols = [
    "Global_Accuracy(%)",
    "Global_Latency_Sum(ms)"
]
# Columns to drop from features
explicit_drop_cols = {
    "Combination",
    "Dataset",
    "Unnamed: 0",
    "Global_RoundTime_Max(ms)",
    "Global_DataVolume"
}

# Validate targets exist
missing_targets = [c for c in target_cols if c not in df.columns]
if missing_targets:
    raise ValueError(f"Missing target columns in CSV: {missing_targets}")

# ================================
# 2. Prepare Data
# ================================
def coerce_num(series):
    return pd.to_numeric(series, errors="coerce")

df_num = df.copy()
for c in df_num.columns:
    df_num[c] = coerce_num(df_num[c])

drop_cols = set(target_cols) | explicit_drop_cols
feature_cols = [
    c for c in df_num.columns
    if c not in drop_cols and pd.api.types.is_numeric_dtype(df_num[c])
]
X = df_num[feature_cols].copy().fillna(df_num[feature_cols].median(numeric_only=True))
Y = df_num[target_cols].copy().fillna(df_num[target_cols].median(numeric_only=True))
combos = df["Combination"] if "Combination" in df.columns else pd.Series([None] * len(df))
datasets = df["Dataset"] if "Dataset" in df.columns else pd.Series(["Unknown"] * len(df))
# ================================
# 3. Train-Test Split
# ================================
X_train, X_test, y_train, y_test, combos_train, combos_test, dataset_train, dataset_test = train_test_split(
    X.values, Y.values, combos.values, datasets.values, test_size=0.2, random_state=42
)

# ================================
# 4. Decision Tree Regression Model
# ================================
# Note: DecisionTreeRegressor supports multi-output regression directly when y is 2D.
model = DecisionTreeRegressor(
    random_state=42,
    max_depth=None,        # you can tune this
    min_samples_split=2,   # you can tune this
    min_samples_leaf=1     # you can tune this
)

model.fit(X_train, y_train)

# ================================
# 5. Evaluation
# ================================
pred_test = model.predict(X_test)
y_test_inv = y_test  # no scaling, so "inv" is just y_test

# ================================
# 6. Overall Metrics (per target)
# ================================
mae  = mean_absolute_error(y_test_inv, pred_test, multioutput="raw_values")
mape = mean_absolute_percentage_error(y_test_inv, pred_test, multioutput="raw_values") * 100
r2   = [r2_score(y_test_inv[:, i], pred_test[:, i]) for i in range(y_test_inv.shape[1])]

metrics_df = pd.DataFrame({
    "Target": target_cols,
    "MAE": mae,
    "MAPE": mape,
    "R2": r2
}).round(4)

# ================================
# 6B. Dataset-wise Aggregated Metrics
# ================================
dataset_metrics = []
unique_datasets = np.unique(dataset_test)

for ds in unique_datasets:
    mask = dataset_test == ds
    y_true_ds = y_test_inv[mask]
    y_pred_ds = pred_test[mask]
    if len(y_true_ds) == 0:
        continue

    # Aggregate across all target dimensions
    mae_ds  = mean_absolute_error(y_true_ds, y_pred_ds)
    mape_ds = mean_absolute_percentage_error(y_true_ds, y_pred_ds) * 100
    r2_ds   = r2_score(y_true_ds.flatten(), y_pred_ds.flatten())

    dataset_metrics.append({
        "Dataset": ds,
        "MAE": mae_ds,
        "MAPE": mape_ds,
        "R2": r2_ds
    })

dataset_metrics_df = pd.DataFrame(dataset_metrics).round(4)

# ================================
# 7. Comparison DataFrame (with dataset info)
# ================================
comparison_df = pd.DataFrame({
    "Dataset": dataset_test,
    "Combination": combos_test,
    "Actual_Accuracy(%)":        y_test_inv[:, 0],
    "Predicted_Accuracy(%)":     pred_test[:, 0],
    "Actual_Latency_Sum(ms)":    y_test_inv[:, 1],
    "Predicted_Latency_Sum(ms)": pred_test[:, 1],
})

# ================================
# 8. Save + Display
# ================================
metrics_path = "decisiontree_metrics.csv"
preds_path   = "decisiontree_test_predictions.csv"
dataset_metrics_path = "datasetwise_decisiontree_metrics.csv"

metrics_df.to_csv(metrics_path, index=False)
dataset_metrics_df.to_csv(dataset_metrics_path, index=False)
comparison_df.to_csv(preds_path, index=False)

print("âœ… Trained on:", csv_path)
print(f"Features used (X): {len(feature_cols)}")
print(f"Targets (Y): {target_cols}\n")
print("Overall Metrics (MAE, MAPE, RÂ²):\n", metrics_df)
print("\nDataset-wise Aggregated Metrics:\n", dataset_metrics_df)
print("\nFull test predictions saved to:", preds_path)

import os
import time
import numpy as np
import pandas as pd

# ================================
# 1. Load New Dataset
# ================================
new_csv_path = "/content/few_text_dataset_1CIFAR10.csv"
assert os.path.exists(new_csv_path), f"File not found: {new_csv_path}"
df_new = pd.read_csv(new_csv_path)

# ================================
# 2. Feature Columns (same as training)
# ================================
feature_cols = [
    'Unnamed: 0',
    'C1_DataVolume(MB)', 'C1_FeatureCount', 'C1_Accuracy(%)', 'C1_Latency(ms)',
    'C1_Label0', 'C1_Label1', 'C1_Label2', 'C1_Label3', 'C1_Label4', 'C1_Label5',
    'C1_Label6', 'C1_Label7', 'C1_Label8', 'C1_Label9',
    'C2_DataVolume(MB)', 'C2_FeatureCount', 'C2_Accuracy(%)', 'C2_Latency(ms)',
    'C2_Label0', 'C2_Label1', 'C2_Label2', 'C2_Label3', 'C2_Label4', 'C2_Label5',
    'C2_Label6', 'C2_Label7', 'C2_Label8', 'C2_Label9',
    'C3_DataVolume(MB)', 'C3_FeatureCount', 'C3_Accuracy(%)', 'C3_Latency(ms)',
    'C3_Label0', 'C3_Label1', 'C3_Label2', 'C3_Label3', 'C3_Label4', 'C3_Label5',
    'C3_Label6', 'C3_Label7', 'C3_Label8', 'C3_Label9',
    'C4_DataVolume(MB)', 'C4_FeatureCount', 'C4_Accuracy(%)', 'C4_Latency(ms)',
    'C4_Label0', 'C4_Label1', 'C4_Label2', 'C4_Label3', 'C4_Label4', 'C4_Label5',
    'C4_Label6', 'C4_Label7', 'C4_Label8', 'C4_Label9',
    'C5_DataVolume(MB)', 'C5_FeatureCount', 'C5_Accuracy(%)', 'C5_Latency(ms)',
    'C5_Label0', 'C5_Label1', 'C5_Label2', 'C5_Label3', 'C5_Label4', 'C5_Label5',
    'C5_Label6', 'C5_Label7', 'C5_Label8', 'C5_Label9',
    'Data_Composability_Score', 'Scalability_Score', 'Accuracy_Similarity_Score'
]

# Check all required columns exist
missing_cols = [c for c in feature_cols if c not in df_new.columns]
if missing_cols:
    raise ValueError(f"Missing columns in new dataset: {missing_cols}")

# ================================
# 3. Preprocess
# ================================
df_new_num = df_new[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
X_new = df_new_num.values

# If (and only if) you trained the Decision Tree using scaler_X,
# then apply it here too. Otherwise, delete the next line.
# X_new = scaler_X.transform(X_new)

# ================================
# 4. Predict row-by-row + timing
# ================================
preds_list = []
times_list = []

for i in range(len(X_new)):
    x_row = X_new[i].reshape(1, -1)
    start_time = time.time()
    pred = model.predict(x_row)          # shape (1, 2)
    end_time = time.time()

    preds_list.append(pred[0])
    times_list.append(end_time - start_time)

preds_arr = np.array(preds_list)

# ================================
# 5. Build Output DataFrame
# ================================
target_cols = ["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]
new_preds_df = pd.DataFrame(preds_arr, columns=target_cols)
new_preds_df["Prediction_Time(s)"] = times_list

# Always include 'Combination'
if "Combination" in df_new.columns:
    new_preds_df.insert(0, "Combination", df_new["Combination"])
else:
    new_preds_df.insert(0, "Combination", [f"Row_{i}" for i in range(len(df_new))])

# Include New_Combination if exists
if "New_Combination" in df_new.columns:
    new_preds_df["New_Combination"] = df_new["New_Combination"]

# ================================
# 6. Save + Print
# ================================
output_path = "new_dataset_predictions_decisiontree.csv"
new_preds_df.to_csv(output_path, index=False)

print(f"âœ… Decision Tree predictions completed for: {new_csv_path}")
print(f"Predictions saved to: {output_path}")
print("\nSample predictions:\n", new_preds_df.head(10))

import pandas as pd
import re
new_preds_df.to_csv("zero-shot_1.csv")
# Load both CSVs
df_sim = pd.read_csv("/content/Simulated_Results_From_New_Combinations_ms.csv")
df_zero = pd.read_csv("zero-shot_1.csv")
df_zero

df_sim

df_zero.rename(columns={"Combination": "'New_Combination"}, inplace=True)

import pandas as pd
import re
# Ensure required column exists
if "New_Combination" not in df.columns:
    raise ValueError("The CSV must have a column named 'New_Combination'.")

# Convert like "C7-C10-C16" â†’ "7-10-16"
def to_numerical_combo(combo):
    if pd.isna(combo):
        return None
    numbers = re.findall(r'C(\d+)', str(combo))
    return "-".join(numbers)
# Apply conversion
df_sim["Numerical_Combination"] = df_sim["New_Combination"].apply(to_numerical_combo)
# Show preview
df_sim
df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_MINIST.csv")

# The issue likely arises because both datasets have duplicate combinations.
# Weâ€™ll ensure unique matches by dropping duplicates before merging.

# Convert list to tuple for matching
df_zero["Numeric_Tuple"] = df_zero["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))
df_sim["Numeric_Tuple"] = df_sim["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))

# Drop duplicates to ensure one-to-one matching
df_zero_unique = df_zero.drop_duplicates(subset=["Numeric_Tuple"])
df_sim_unique = df_sim.drop_duplicates(subset=["Numeric_Tuple"])

# Find intersection (only unique tuples that appear in both)
matched_tuples = set(df_zero_unique["Numeric_Tuple"]).intersection(set(df_sim_unique["Numeric_Tuple"]))

# Filter only those rows
df_zero_matched = df_zero_unique[df_zero_unique["Numeric_Tuple"].isin(matched_tuples)]
df_sim_matched = df_sim_unique[df_sim_unique["Numeric_Tuple"].isin(matched_tuples)]

# Merge one-to-one based on the unique tuple
merged_df = pd.merge(df_zero_matched, df_sim_matched, on="Numeric_Tuple", suffixes=("_zero", "_sim"))

# Save final merged result
output_path = "Matched_Combinations_Results_Unique.csv"
merged_df.to_csv(output_path, index=False)

output_path, merged_df.shape

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_absolute_error,
    mean_absolute_percentage_error,
    r2_score,
    mean_squared_error,
    median_absolute_error,
    explained_variance_score,
    max_error
)

# --- SMAPE function ---
def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
def compute_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    medae = median_absolute_error(y_true, y_pred)
    evs = explained_variance_score(y_true, y_pred)
    smape_val = smape(y_true.values, y_pred.values)
    maxerr = max_error(y_true, y_pred)

    return mae, mape, mse, rmse, r2, medae, evs, smape_val, maxerr


metrics_acc = compute_metrics(y_true_acc, y_pred_acc)
metrics_lat = compute_metrics(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])
metrics_all = compute_metrics(pd.Series(y_true_all), pd.Series(y_pred_all))

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [metrics_acc[0], metrics_lat[0], metrics_all[0]],
    "MAPE": [metrics_acc[1], metrics_lat[1], metrics_all[1]],
    "MSE": [metrics_acc[2], metrics_lat[2], metrics_all[2]],
    "RMSE": [metrics_acc[3], metrics_lat[3], metrics_all[3]]
}).round(4)

metrics_df

0.2715 &11.4967 &0.1468 &1360.4197 &
0.4327&13.72370.86545340.1538

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
merged_df=merged_df_updated
# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# Calculate per-target metrics
mae_acc = mean_absolute_error(y_true_acc, y_pred_acc)
mape_acc = mean_absolute_percentage_error(y_true_acc, y_pred_acc)
r2_acc = r2_score(y_true_acc, y_pred_acc)

mae_lat = mean_absolute_error(y_true_lat, y_pred_lat)
mape_lat = mean_absolute_percentage_error(y_true_lat, y_pred_lat)
r2_lat = r2_score(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
# Stack the two sets of predictions into single arrays
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])

mae_overall = mean_absolute_error(y_true_all, y_pred_all)
mape_overall = mean_absolute_percentage_error(y_true_all, y_pred_all)
r2_overall = r2_score(y_true_all, y_pred_all)

# === Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["FMINIST"],
    "MAE": [mae_overall],
    "MAPE": [mape_overall],
    "R2": [r2_overall]
}).round(4)

metrics_df

merged_df_updated.isna().sum()

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

# ===============================
# Load and prepare data
# ===============================
df = pd.read_csv("merged_with_historical.csv").dropna(
    subset=["ZeroShot_Accuracy(%)", "Simulated_Accuracy(%)"]
).reset_index(drop=True)

df["Combination_Index"] = range(len(df))

# ===============================
# Plot configuration
# ===============================
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_context("talk", font_scale=1.5)

plt.figure(figsize=(14, 7))

# Zero-shot predicted accuracy
sns.lineplot(
    x="Combination_Index", y="ZeroShot_Accuracy(%)", data=df,
    label="Zero-Shot Predicted", color="royalblue", linewidth=3
)

# Simulated (final predicted) accuracy
sns.lineplot(
    x="Combination_Index", y="Simulated_Accuracy(%)", data=df,
    label="Simulated Accuracy", color="darkorange", linewidth=3
)

# ===============================
# Formatting
# ===============================
plt.xlabel("Combination Index", fontsize=22, labelpad=15)
plt.ylabel("Accuracy (%)", fontsize=22, labelpad=15)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.grid(True, which='major', linestyle='--', linewidth=0.6, alpha=0.6)
plt.legend(fontsize=20, loc='best', frameon=False)
plt.tight_layout()

# ===============================
# Optional high-res save
# ===============================
# plt.savefig("zero_vs_simulated_accuracy_acm.pdf", dpi=600, bbox_inches='tight')

plt.show()

# ===============================
# Compute and print trend consistency
# ===============================
df["ZS_Trend"] = np.sign(df["ZeroShot_Accuracy(%)"].diff())
df["Sim_Trend"] = np.sign(df["Simulated_Accuracy(%)"].diff())
trend_agreement = (df["ZS_Trend"] == df["Sim_Trend"]).mean() * 100
print(f"Trend Consistency between Zero-Shot and Simulated Accuracies: {trend_agreement:.2f}%")

"""**DT-QoS MINIST**
---
"""

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

# ================================
# 1. Load Dataset
# ================================
csv_path = "Zero_shot_Dataset.csv"
assert os.path.exists(csv_path), f"File not found: {csv_path}"
df = pd.read_csv(csv_path)
# ================================
# Target / Exclusions
# ================================
target_cols = [
    "Global_Accuracy(%)",
    "Global_Latency_Sum(ms)"
]
# Columns to drop from features
explicit_drop_cols = {
    "Combination",
    "Dataset",
    "Unnamed: 0",
    "Global_RoundTime_Max(ms)",
    "Global_DataVolume"
}

# Validate targets exist
missing_targets = [c for c in target_cols if c not in df.columns]
if missing_targets:
    raise ValueError(f"Missing target columns in CSV: {missing_targets}")

# ================================
# 2. Prepare Data
# ================================
def coerce_num(series):
    return pd.to_numeric(series, errors="coerce")

df_num = df.copy()
for c in df_num.columns:
    df_num[c] = coerce_num(df_num[c])

drop_cols = set(target_cols) | explicit_drop_cols
feature_cols = [
    c for c in df_num.columns
    if c not in drop_cols and pd.api.types.is_numeric_dtype(df_num[c])
]
X = df_num[feature_cols].copy().fillna(df_num[feature_cols].median(numeric_only=True))
Y = df_num[target_cols].copy().fillna(df_num[target_cols].median(numeric_only=True))
combos = df["Combination"] if "Combination" in df.columns else pd.Series([None] * len(df))
datasets = df["Dataset"] if "Dataset" in df.columns else pd.Series(["Unknown"] * len(df))
# ================================
# 3. Train-Test Split
# ================================
X_train, X_test, y_train, y_test, combos_train, combos_test, dataset_train, dataset_test = train_test_split(
    X.values, Y.values, combos.values, datasets.values, test_size=0.2, random_state=42
)

# ================================
# 4. Decision Tree Regression Model
# ================================
# Note: DecisionTreeRegressor supports multi-output regression directly when y is 2D.
model = DecisionTreeRegressor(
    random_state=42,
    max_depth=None,        # you can tune this
    min_samples_split=2,   # you can tune this
    min_samples_leaf=1     # you can tune this
)

model.fit(X_train, y_train)

# ================================
# 5. Evaluation
# ================================
pred_test = model.predict(X_test)
y_test_inv = y_test  # no scaling, so "inv" is just y_test

# ================================
# 6. Overall Metrics (per target)
# ================================
mae  = mean_absolute_error(y_test_inv, pred_test, multioutput="raw_values")
mape = mean_absolute_percentage_error(y_test_inv, pred_test, multioutput="raw_values") * 100
r2   = [r2_score(y_test_inv[:, i], pred_test[:, i]) for i in range(y_test_inv.shape[1])]

metrics_df = pd.DataFrame({
    "Target": target_cols,
    "MAE": mae,
    "MAPE": mape,
    "R2": r2
}).round(4)

# ================================
# 6B. Dataset-wise Aggregated Metrics
# ================================
dataset_metrics = []
unique_datasets = np.unique(dataset_test)

for ds in unique_datasets:
    mask = dataset_test == ds
    y_true_ds = y_test_inv[mask]
    y_pred_ds = pred_test[mask]
    if len(y_true_ds) == 0:
        continue

    # Aggregate across all target dimensions
    mae_ds  = mean_absolute_error(y_true_ds, y_pred_ds)
    mape_ds = mean_absolute_percentage_error(y_true_ds, y_pred_ds) * 100
    r2_ds   = r2_score(y_true_ds.flatten(), y_pred_ds.flatten())

    dataset_metrics.append({
        "Dataset": ds,
        "MAE": mae_ds,
        "MAPE": mape_ds,
        "R2": r2_ds
    })

dataset_metrics_df = pd.DataFrame(dataset_metrics).round(4)

# ================================
# 7. Comparison DataFrame (with dataset info)
# ================================
comparison_df = pd.DataFrame({
    "Dataset": dataset_test,
    "Combination": combos_test,
    "Actual_Accuracy(%)":        y_test_inv[:, 0],
    "Predicted_Accuracy(%)":     pred_test[:, 0],
    "Actual_Latency_Sum(ms)":    y_test_inv[:, 1],
    "Predicted_Latency_Sum(ms)": pred_test[:, 1],
})

# ================================
# 8. Save + Display
# ================================
metrics_path = "decisiontree_metrics.csv"
preds_path   = "decisiontree_test_predictions.csv"
dataset_metrics_path = "datasetwise_decisiontree_metrics.csv"

metrics_df.to_csv(metrics_path, index=False)
dataset_metrics_df.to_csv(dataset_metrics_path, index=False)
comparison_df.to_csv(preds_path, index=False)

print("âœ… Trained on:", csv_path)
print(f"Features used (X): {len(feature_cols)}")
print(f"Targets (Y): {target_cols}\n")
print("Overall Metrics (MAE, MAPE, RÂ²):\n", metrics_df)
print("\nDataset-wise Aggregated Metrics:\n", dataset_metrics_df)
print("\nFull test predictions saved to:", preds_path)

import os
import time
import numpy as np
import pandas as pd

# ================================
# 1. Load New Dataset
# ================================
new_csv_path = "few_text_dataset_minist.csv"
assert os.path.exists(new_csv_path), f"File not found: {new_csv_path}"
df_new = pd.read_csv(new_csv_path)

# ================================
# 2. Feature Columns (same as training)
# ================================
feature_cols = [
    'Unnamed: 0',
    'C1_DataVolume(MB)', 'C1_FeatureCount', 'C1_Accuracy(%)', 'C1_Latency(ms)',
    'C1_Label0', 'C1_Label1', 'C1_Label2', 'C1_Label3', 'C1_Label4', 'C1_Label5',
    'C1_Label6', 'C1_Label7', 'C1_Label8', 'C1_Label9',
    'C2_DataVolume(MB)', 'C2_FeatureCount', 'C2_Accuracy(%)', 'C2_Latency(ms)',
    'C2_Label0', 'C2_Label1', 'C2_Label2', 'C2_Label3', 'C2_Label4', 'C2_Label5',
    'C2_Label6', 'C2_Label7', 'C2_Label8', 'C2_Label9',
    'C3_DataVolume(MB)', 'C3_FeatureCount', 'C3_Accuracy(%)', 'C3_Latency(ms)',
    'C3_Label0', 'C3_Label1', 'C3_Label2', 'C3_Label3', 'C3_Label4', 'C3_Label5',
    'C3_Label6', 'C3_Label7', 'C3_Label8', 'C3_Label9',
    'C4_DataVolume(MB)', 'C4_FeatureCount', 'C4_Accuracy(%)', 'C4_Latency(ms)',
    'C4_Label0', 'C4_Label1', 'C4_Label2', 'C4_Label3', 'C4_Label4', 'C4_Label5',
    'C4_Label6', 'C4_Label7', 'C4_Label8', 'C4_Label9',
    'C5_DataVolume(MB)', 'C5_FeatureCount', 'C5_Accuracy(%)', 'C5_Latency(ms)',
    'C5_Label0', 'C5_Label1', 'C5_Label2', 'C5_Label3', 'C5_Label4', 'C5_Label5',
    'C5_Label6', 'C5_Label7', 'C5_Label8', 'C5_Label9',
    'Data_Composability_Score', 'Scalability_Score', 'Accuracy_Similarity_Score'
]

# Check all required columns exist
missing_cols = [c for c in feature_cols if c not in df_new.columns]
if missing_cols:
    raise ValueError(f"Missing columns in new dataset: {missing_cols}")

# ================================
# 3. Preprocess
# ================================
df_new_num = df_new[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
X_new = df_new_num.values

# If (and only if) you trained the Decision Tree using scaler_X,
# then apply it here too. Otherwise, delete the next line.
# X_new = scaler_X.transform(X_new)

# ================================
# 4. Predict row-by-row + timing
# ================================
preds_list = []
times_list = []

for i in range(len(X_new)):
    x_row = X_new[i].reshape(1, -1)
    start_time = time.time()
    pred = model.predict(x_row)          # shape (1, 2)
    end_time = time.time()

    preds_list.append(pred[0])
    times_list.append(end_time - start_time)

preds_arr = np.array(preds_list)

# ================================
# 5. Build Output DataFrame
# ================================
target_cols = ["Global_Accuracy(%)", "Global_Latency_Sum(ms)"]
new_preds_df = pd.DataFrame(preds_arr, columns=target_cols)
new_preds_df["Prediction_Time(s)"] = times_list

# Always include 'Combination'
if "Combination" in df_new.columns:
    new_preds_df.insert(0, "Combination", df_new["Combination"])
else:
    new_preds_df.insert(0, "Combination", [f"Row_{i}" for i in range(len(df_new))])

# Include New_Combination if exists
if "New_Combination" in df_new.columns:
    new_preds_df["New_Combination"] = df_new["New_Combination"]

# ================================
# 6. Save + Print
# ================================
output_path = "new_dataset_predictions_decisiontree.csv"
new_preds_df.to_csv(output_path, index=False)

print(f"âœ… Decision Tree predictions completed for: {new_csv_path}")
print(f"Predictions saved to: {output_path}")
print("\nSample predictions:\n", new_preds_df.head(10))

import pandas as pd
import re
new_preds_df.to_csv("zero-shot_1.csv")
# Load both CSVs
df_sim = pd.read_csv("/content/Simulated_Results_From_New_Combinations_ms.csv")
df_zero = pd.read_csv("zero-shot_1.csv")
df_zero

df_sim

df_zero.rename(columns={"Combination": "'New_Combination"}, inplace=True)

import pandas as pd
import re
# Ensure required column exists
if "New_Combination" not in df.columns:
    raise ValueError("The CSV must have a column named 'New_Combination'.")

# Convert like "C7-C10-C16" â†’ "7-10-16"
def to_numerical_combo(combo):
    if pd.isna(combo):
        return None
    numbers = re.findall(r'C(\d+)', str(combo))
    return "-".join(numbers)
# Apply conversion
df_sim["Numerical_Combination"] = df_sim["New_Combination"].apply(to_numerical_combo)
# Show preview
df_sim
df_sim.to_csv("/content/Simulated_Results_From_New_Combinations_MINIST.csv")

# The issue likely arises because both datasets have duplicate combinations.
# Weâ€™ll ensure unique matches by dropping duplicates before merging.

# Convert list to tuple for matching
df_zero["Numeric_Tuple"] = df_zero["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))
df_sim["Numeric_Tuple"] = df_sim["New_Combination"].apply(lambda x: tuple(sorted([int(i.replace("C", "")) for i in x.split("-")])))

# Drop duplicates to ensure one-to-one matching
df_zero_unique = df_zero.drop_duplicates(subset=["Numeric_Tuple"])
df_sim_unique = df_sim.drop_duplicates(subset=["Numeric_Tuple"])

# Find intersection (only unique tuples that appear in both)
matched_tuples = set(df_zero_unique["Numeric_Tuple"]).intersection(set(df_sim_unique["Numeric_Tuple"]))

# Filter only those rows
df_zero_matched = df_zero_unique[df_zero_unique["Numeric_Tuple"].isin(matched_tuples)]
df_sim_matched = df_sim_unique[df_sim_unique["Numeric_Tuple"].isin(matched_tuples)]

# Merge one-to-one based on the unique tuple
merged_df = pd.merge(df_zero_matched, df_sim_matched, on="Numeric_Tuple", suffixes=("_zero", "_sim"))

# Save final merged result
output_path = "Matched_Combinations_Results_Unique.csv"
merged_df.to_csv(output_path, index=False)

output_path, merged_df.shape

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_absolute_error,
    mean_absolute_percentage_error,
    r2_score,
    mean_squared_error,
    median_absolute_error,
    explained_variance_score,
    max_error
)

# --- SMAPE function ---
def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

# Extract y_true and y_pred for both targets
y_true_acc = merged_df['Global_Accuracy(%)_zero']
y_pred_acc = merged_df['Global_Accuracy(%)_sim']
y_true_lat = merged_df['Global_Latency_Sum(ms)_zero']
y_pred_lat = merged_df['Global_Latency_Sum(ms)_sim']

# === ðŸ”¹ Per-target metrics ===
def compute_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    medae = median_absolute_error(y_true, y_pred)
    evs = explained_variance_score(y_true, y_pred)
    smape_val = smape(y_true.values, y_pred.values)
    maxerr = max_error(y_true, y_pred)

    return mae, mape, mse, rmse, r2, medae, evs, smape_val, maxerr


metrics_acc = compute_metrics(y_true_acc, y_pred_acc)
metrics_lat = compute_metrics(y_true_lat, y_pred_lat)

# === ðŸ”¹ Overall metrics (combined across both targets) ===
y_true_all = np.concatenate([y_true_acc.values, y_true_lat.values])
y_pred_all = np.concatenate([y_pred_acc.values, y_pred_lat.values])
metrics_all = compute_metrics(pd.Series(y_true_all), pd.Series(y_pred_all))

# === ðŸ”¹ Create summary DataFrame ===
metrics_df = pd.DataFrame({
    "Target": ["Accuracy", "Latency", "Overall"],
    "MAE": [metrics_acc[0], metrics_lat[0], metrics_all[0]],
    "MAPE": [metrics_acc[1], metrics_lat[1], metrics_all[1]],
    "MSE": [metrics_acc[2], metrics_lat[2], metrics_all[2]],
    "RMSE": [metrics_acc[3], metrics_lat[3], metrics_all[3]]
}).round(4)

metrics_df